# 神经网络
> 一种强大的计算模型

也称人工神经网络ANN(Artificial Neural Network)，是20世纪80年代以来人工智能领域的一个研究热点。
它是对人脑神经元网络进行抽象化处理，建立的一种简单模型，并按不同的连接方式组成不同的网络。

神经网络由大量的节点(神经元)相互连接构成，每个节点代表一种特定的输出函数，称为激励函数(activation function)。激励函数的作用使得神经元具有非线性性，以更好的模拟实际问题的复杂性。
每两个节点之间的连接都代表一个对于通过该连接信号的加权值，称之为权重，相当于人工神经网络的记忆。

网络的输出则依赖连接方式、权重值和激励函数的不同而不同。工作就是输入数据，处理数据，给出一个结果作为输出。

神经网络一般分成三层，
- 输入层，负责接收外部数据
- 隐含层，对数据进行处理    
- 输出层，负责最终结果输出

神经网络的实现有多种

## 前馈神经网络

MLP实际上是一种非常古老的架构，可以追溯到50年代。其设计初衷是模仿大脑结构；由许多互联的神经元组成，这些神经元将信息向前传递，因此得名前馈网络（feed-forward network）。

多层感知器在深度学习模型中无处不在。例如，我们知道它们被用于GPT-2、3以及（可能的）4等模型的Transformer模块之间。对MLP的改进将对机器学习世界产生广泛的影响。

其用示意图展示的并没有很好的理解，用数学公式描述更明显一些,下面是一个两层的MLP

$$
h_{1} = W_{1}x + b_{1} \newline
f_{1} = f(h_{1}) \newline
h_{2} = W_{2}f_{1} + b_{2} \newline
y = f(h_{2})
$$

其中W是可学习权重的矩阵，b是偏差向量。函数f是一个非线性函数。看到这些方程，很明显，一个MLP是一系列带有非线性间隔的线性回归模型。这是一个非常基本的设置。

尽管基本，但它表达力极强。有数学保证，MLP是通用逼近器，即：它们可以逼近任何函数，类似于所有函数都可以用泰勒级数来表示。

为了训练模型的权重，我们使用了反向传播（backpropagation），这要归功于自动微分(autodiff)。我不会在这里深入讨论，但重要的是要注意自动微分可以对任何可微函数起作用，这在后面会很重要。

存在一些严重的缺点
- 因为它们作为模型极其灵活，可以很好地适应任何数据。结果，它们很可能过拟合
- 模型中往往包含大量的权重，解释这些权重以从数据中得出结论变得非常困难。我们常说深度学习模型是“黑盒”
- 拥有大量的权重还意味着它们的训练可能会很长，GPT-3的大部分参数都在MLP层中

## 反馈神经网络

将当前输出再次接到输入层，使得输入层不仅仅取上一层节点的输出

## RNN(Recurrent Neural Network)

循环神经网络，是一种递归式的神经网络结构，通过在序列的每个位置上共享权重，将当前位置的输入与前一时刻的隐藏状态进行串联处理，是一种用于处理序列数据的神经网络。
RNN能记住之前的信息，并将其应用于当前的计算中，这使得处理自然语言、语音识别、时间序列预测等数据有较好的优势。

RNN的缺陷有梯度消失和梯度爆炸，会导致处理长序列数据时性能下降，为解决这个问题提出了改进方法
- 长短时记忆网络LSTM
- 门控循环单牙GRU

## [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/abs/2404.19756)

- [github code](https://github.com/KindXiaoming/pykan)
- [Awesome KAN(Kolmogorov-Arnold Network)](https://github.com/mintisan/awesome-kan)
- [解读](https://mp.weixin.qq.com/s/5wZuw4fJWsouEvsQ5r2zMw)

Kolmogorov-Arnold 网络（KAN）是一种全新的神经网络构建块。它比多层感知器（MLP）更具表达力、更不易过拟合且更易于解释。

Kolmogorov-Arnold 表示定理的目标类似于支撑MLP的通用逼近定理，但前提不同。它本质上说，任何多变量函数都可以用1维非线性函数的加法来表示。

这为我们提供了一种不同但简单的范式来开始构建神经网络架构。作者声称，这种架构比使用多层感知器（MLP）更易于解释、更高效地使用参数，并且具有更好的泛化能力。在MLP中，非线性函数是固定的，在训练过程中从未改变。而在KAN中，不再有权重矩阵或偏差，只有适应数据的一维非线性函数。然后将这些非线性函数相加。我们可以堆叠越来越多的层来创建更复杂的函数。

## Transformer
是一种基于自注意力的序列建模模型，它的每层都由多头注意力机制和前馈神经网络组成，注意力机制来建模序列中的依赖关系，每个输入元素都与序列中的其他元素进行交互，并根据交互结果来调整自身的表示。

Transformer因并行和长期依赖建模优势，在自然语言领域表现出色，

- [3D 可视化 GPT 大语言模型](https://bbycroft.net/llm)
- [神经网络算法 —— 一文搞懂Transformer ](https://developer.aliyun.com/article/1462200?utm_content=g_1000391552&accounttraceid=5b0de7cbee3d4e04b732246795313dcegdxe)

